from .model_client import make_client
from .diff_utils import extract_changed_files, chunk_text
from .prompts import build_prompts


def single_model_review(cfg: dict, model_name: str, diff_text: str, pr_meta: dict | None) -> str:
    client = make_client(cfg)

    files = extract_changed_files(diff_text)
    file_list_text = (
        "Files changed:\n" + "\n".join(f"- {f}" for f in files)
        if files else "Files changed: (not detected)"
    )

    chunks = chunk_text(diff_text, max_chars=12000)
    system, user_template, format_hint = build_prompts(cfg)
    all_parts: list[str] = []

    for i, chunk in enumerate(chunks, 1):
        meta_lines: list[str] = []
        if pr_meta:
            meta_lines.append(f"PR Title: {pr_meta.get('title','')}")
            meta_lines.append(f"Author: {(pr_meta.get('user') or {}).get('login','')}")
            meta_lines.append(
                f"Base → Head: {(pr_meta.get('base') or {}).get('ref','')} → {(pr_meta.get('head') or {}).get('ref','')}"
            )

        header_block = "\n\n".join(
            filter(
                None,
                [
                    "\n".join(meta_lines),
                    file_list_text,
                    user_template,
                    format_hint,
                ],
            )
        )

        completion = client.chat.completions.create(
            extra_headers={"x-correlation-id": (cfg.get("correlation_id") or "pr-review-ui")},
            model=model_name,
            messages=[
                {"role": "system", "content": system},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": header_block},
                        {"type": "text", "text": f"(Chunk {i}/{len(chunks)})"},
                        {"type": "text", "text": f"```diff\n{chunk}\n```"},
                    ],
                },
            ],
            stream=False,
            temperature=0.2,
        )
        all_parts.append(completion.choices[0].message.content)

    if len(all_parts) == 1:
        return all_parts[0]

    consolidated_prompt = (
        "Merge these chunked reviews into one. "
        + (
            "Return a single HTML fragment only (no <html> wrapper)."
            if (cfg.get("output_format", "html") == "html")
            else "Follow the Markdown format strictly."
        )
        + " Deduplicate and merge by file. Produce one Change Summary, one Review Table, and one Overall Verdict."
    )

    completion = client.chat.completions.create(
        extra_headers={"x-correlation-id": (cfg.get("correlation_id") or "pr-review-ui")},
        model=model_name,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": consolidated_prompt},
            {"role": "user", "content": "\n\n".join(all_parts)},
        ],
        stream=False,
        temperature=0.2,
    )
    return completion.choices[0].message.content


def synthesize_with_base(cfg: dict, base_model: str, reviews_by_model: dict) -> str:
    client = make_client(cfg)
    system, user_template, format_hint = build_prompts(cfg)

    sources: list[str] = []
    for m, content in reviews_by_model.items():
        if not content:
            continue
        # Keep a newline between the model header and its content
        sources.append(f"### Model: {m}\n{content}")

    synth_user = (
        "You are given multiple code review drafts generated by different models for the SAME PR. "
        "Synthesize a SINGLE best review that strictly follows the required format already provided in the template, "
        "preserves the 'Change Requirement' section (High-Level Summary + Acceptance Criteria), and then "
        "continues with Change Summary by File, Review Table, and Overall Verdict. "
        "Resolve conflicts by preferring well-justified, concrete issues and precise recommendations. "
        "Be concise, remove duplicates, and ensure the final output is internally consistent and complete."
    )

    completion = client.chat.completions.create(
        extra_headers={"x-correlation-id": (cfg.get("correlation_id") or "pr-review-ui")},
        model=base_model,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user_template},
            {"role": "user", "content": format_hint},
            {"role": "user", "content": synth_user},
            {"role": "user", "content": "\n\n".join(sources) if sources else "No sources available."},
        ],
        stream=False,
        temperature=0.2,
    )
    return completion.choices[0].message.content
